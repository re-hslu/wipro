Im Projektauftrag der WIPRO ist festgelegt, dass die Android-Applikation nach
einem \emph{AI-First}-Ansatz entwickelt werden soll. Ziel ist es dabei, künstliche
Intelligenz bewusst und aktiv in den Entwicklungsprozess einzubinden und deren
Nutzen im realen Projektalltag zu prüfen.

Damit dieser Ansatz sinnvoll umgesetzt werden kann, ist eine Auswahl
geeigneter AI-Tools notwendig. Da eine Vielzahl unterschiedlicher KI-Lösungen
existiert und nicht für jedes Tool ein kostenpflichtiges Abonnement abgeschlossen
werden kann, wurde zu Beginn eine kleine Evaluation durchgeführt. Ziel dieser
Evaluation ist es, jene Tools zu identifizieren, die im Kontext der Android-
Entwicklung den grössten praktischen Mehrwert bieten.

Die Evaluation basiert auf einer Kombination aus vorhandenen
Erkenntnissen sowie praktischen Erfahrungen aus Selbstexperimenten mit den
kostenlosen Versionen der jeweiligen Tools. Nach Abschluss dieser Analyse wurde
eine fundierte Entscheidung für ein primäres Tool getroffen, welches im weiteren
Projektverlauf auch in der kostenpflichtigen Version eingesetzt wird, um den
AI-First-Ansatz konsequent und effektiv umzusetzen.

\subsection{Vorarbeiten}

Im Vorfeld dieses Wirtschaftsprojekts wurde im Rahmen des Moduls
\emph{Wissenschaftliches Schreiben \& Forschungsmethodik (WSFM)} im
September 2025 bereits untersucht, wie der Einsatz von AI-Tools im
Kontext der Softwareentwicklung sinnvoll und methodisch korrekt
evaluiert werden kann. Ziel dieser Vorarbeit war es, zu klären, mit
welchen Ansätzen sich der Nutzen von AI-Tools messen lässt und wie eine
mögliche Zeitersparnis wissenschaftlich belegt werden könnte
(siehe Anhang~\ref{anhang:wsfm}).

Das Ergebnis dieser Vorarbeit war die Erkenntnis, dass ein
streng wissenschaftlicher Ansatz mit Umfragen, kontrollierten
Experimenten, Tests mit erfahrenen Entwicklerinnen sowie Interviews
einen sehr hohen organisatorischen und zeitlichen Aufwand erfordern
würde. Für den Rahmen dieses Wirtschaftsprojekts hätte ein solches
Vorgehen den Umfang deutlich überschritten und wäre nicht realistisch
umsetzbar gewesen.

Diese Einschätzung wurde auch im Austausch mit dem Auftraggeber
diskutiert und gemeinsam im Projektstatus-Meeting vom 25.09.2025
bestätigt (siehe Anhang~\ref{anhang:meeting25092025}). Aus diesem Grund
wurde bewusst auf eine vollständig wissenschaftliche Evaluation
verzichtet.

Stattdessen wurde ein pragmatischer, praxisnaher Ansatz gewählt. Die
Evaluation der AI-Tools erfolgte direkt anhand des konkreten
Projektkontexts, insbesondere durch die Übertragung grösserer
Codebestandteile von Swift nach Kotlin. Dieselben Aufgabenstellungen
wurden mehreren AI-Tools vorgelegt und die Resultate schrittweise
beurteilt. Dabei standen für uns Kriterien wie Verständlichkeit,
Korrektheit, Anpassungsaufwand und tatsächlicher Nutzen im
Projektalltag im Vordergrund. Auf dieser Basis konnte ein fundiertes
Fazit gezogen werden, welche Tools sich für unseren Anwendungsfall am
besten eignen (vgl. ebenfalls Anhang~\ref{anhang:meeting25092025}).

\subsection{Aufgabenstellung für die AI Tools}
Wir haben mehreren AI-Tools, die wir entweder bereits selbst genutzt haben,
von denen wir viel Positives gehört haben oder die in wissenschaftlichen
Untersuchungen häufig genannt werden, dieselbe Aufgabenstellung vorgelegt.
Ziel war es, die jeweiligen Antworten zu analysieren und miteinander zu
vergleichen, um einen ersten Eindruck zu erhalten, welches Tool die
umfangreichste und für unseren Anwendungsfall sinnvollste Analyse liefert.

Die vollständige und detaillierte Aufgabenstellung ist im Anhang dokumentiert
(siehe Anhang~\ref{aufgabenstellung-ai-tools}).

\subsection{Analyse der evaluierten AI-Tools}
Im Rahmen der Evaluation wurden die AI-Tools ChatGPT, Grok (xAI), DeepSeek und
Cursor betrachtet. Allen Tools wurde dieselbe Aufgabenstellung vorgegeben, um
ihre Fähigkeiten zur Analyse sowie zur Unterstützung bei der
Android-App-Entwicklung vergleichbar beurteilen zu können.

ChatGPT lieferte eine fundierte Antwort mit ausreichender Tiefe für eine erste
technische Einschätzung. Besonders hilfreich war das breite
Cross-Plattform-Wissen in den Bereichen iOS und Android sowie der Vergleich
zwischen SwiftUI und Jetpack Compose. Zudem wurden sinnvolle Hinweise zu
Architektur und Testing gegeben. Die Pro-Version von ChatGPT kostet 23 Euro pro
Monat. Die vollständige Antwort ist im Anhang dokumentiert
(siehe Anhang~\ref{antwort-chatgpt}).

Auch Grok von xAI erzeugte eine strukturierte und nachvollziehbare Antwort, die
einen guten Überblick über mögliche Lösungsansätze bietet. Dabei wurden
verschiedene technische Optionen aufgezeigt und priorisiert. Mit dem Modell
„grok-code-fast-1“ richtet sich Grok gezielt an Entwickleraufgaben mit Fokus auf
Effizienz und Code-Qualität. Die SuperGrok-Version kostet 30 Euro pro Monat. Die
vollständige Antwort ist im Anhang zu finden
(siehe Anhang~\ref{antwort-grok}).

DeepSeek überzeugte insbesondere durch die detaillierte Auswertung der
Aufgabenstellung sowie der bereitgestellten Screenshots und Codeausschnitte.
Die Antworten enthielten konkrete Entwürfe für die Android-App, ergänzende
Strategien sowie erste Codebeispiele. Zusätzlich wurde eine strukturierte
Checkliste der notwendigen Umsetzungsschritte bereitgestellt. DeepSeek ist
kostenlos nutzbar und bietet keine kostenpflichtigen Abonnements an. Die
vollständige Antwort ist im Anhang dokumentiert
(siehe Anhang~\ref{antwort-deepseek}).

Cursor unterscheidet sich von den anderen Tools dadurch, dass es sich nicht um
eine reine Web-KI handelt, sondern um eine eigenständige Anwendung mit IDE-nahem
Frontend. Ganze Projektstrukturen können geöffnet und analysiert werden, wodurch
der KI jederzeit der vollständige Projektkontext zur Verfügung steht. Cursor
selbst ist kein eigenes Sprachmodell, sondern dient als Oberfläche zur Nutzung
verschiedener LLMs. Dieser Ansatz bietet einen klaren Vorteil gegenüber
klassischen Web-Tools, da Zusammenhänge im Projekt besser erkannt werden können.
Die Pro-Version von Cursor kostet 20 Euro pro Monat. Die vollständige Antwort ist
im Anhang dokumentiert (siehe Anhang~\ref{antwort-cursor}).

\subsection{Vergleich}

\begin{table}[H]
  \centering
  \small
  \setlength{\tabcolsep}{6pt}
  \renewcommand{\arraystretch}{1.2}
  \begin{tabularx}{\textwidth}{l>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X}
    \toprule
    \textbf{KI} &
    \textbf{Stärken / beobachtete Fähigkeiten} &
    \textbf{Schwächen / Unsicherheiten} &
    \textbf{Eignung} \\
    \midrule
    Grok (xAI) &
    xAI hat mit „grok-code-fast-1“ ein Modell für Entwickler-Aufgaben (Agentic Coding) vorgestellt; Effizienz/Qualität im Fokus; funktioniert auch als Backend in Tools wie Cursor. &
    Relativ jung; unklare Reife/Plattformtiefe; unklar, wie gut Apple/Android-SDKs abgedeckt sind. &
    Spannend mit aktueller Grok-Version; mit guter Prompt-Strategie evtl. starker Konkurrent zu Claude. \\
    \addlinespace
    DeepSeek &
    Kostengünstige, vergleichbare Alternative; Integration in IDE-Umgebungen möglich. &
    Weniger öffentlich dokumentierte Benchmarks; evtl. unreifer bei Edge-Cases / Plattformcode. &
    Gute Ergänzung, v. a. wenn Kosten zählen; für kritische Teile menschliches Review einplanen. \\
    \addlinespace
    ChatGPT &
    Sehr stark bei Cross-Plattform-Wissen (iOS/Android, SwiftUI/Compose, Flutter, RN); bewährt; gute Architektur- und Testing-Hinweise. &
    Token-Limits bei sehr grossen Codebasen (ausser Enterprise/Pro); potenzielle Halluzinationen → Review nötig. &
    Top-Option neben Claude; stark für Architektur/Best Practices. \\
    \addlinespace
    Cursor &
    IDE-Frontend, das verschiedene Modelle einbindet; verbessert Workflow und Kontext im Editor. &
    Keine eigene KI; Leistung hängt vom gewählten Modell ab. &
    Als Interface/Workflow-Booster zusammen mit starkem Modell nutzen (z. B. Grok/ChatGPT/Claude). \\
    \bottomrule
  \end{tabularx}
  \caption{Auswertung der KI in Tabellenform}
\end{table}

\subsection{Auswertung anhand von Bewertungskriterien}

Um die evaluierten AI-Tools nicht nur qualitativ zu vergleichen, sondern auch
eine nachvollziehbare Entscheidungsgrundlage zu schaffen, wurden klare
Bewertungskriterien definiert. Diese Kriterien orientieren sich direkt am
Projektalltag und daran, welchen tatsächlichen Mehrwert ein Tool während der
Entwicklung bietet.

Die Bewertung erfolgte subjektiv auf Basis unserer praktischen Erfahrungen im
Projekt. Jedes Kriterium wurde auf einer Skala von 1 (sehr schlecht) bis 5
(sehr gut) beurteilt.

Die folgenden Kriterien wurden für die Auswertung herangezogen:
\begin{itemize}
    \item \textbf{Code-Qualität}: Struktur, Lesbarkeit und Nähe zu Clean-Code-Prinzipien
    \item \textbf{Korrektheit und Funktionsfähigkeit}: fachlich korrekte und lauffähige Resultate
    \item \textbf{Anpassungsaufwand}: benötigte Nacharbeit bis zur produktiven Nutzung
    \item \textbf{Verständnis des Kontexts}: Fähigkeit, Projektstruktur und Zusammenhänge zu erfassen
    \item \textbf{Zeitersparnis im Projektalltag}: tatsächlicher Effizienzgewinn gegenüber manueller Umsetzung
\end{itemize}

\begin{table}[H]
    \centering
    \small
    \setlength{\tabcolsep}{6pt}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabularx}{\textwidth}{lccccc}
        \toprule
        \textbf{KI-Tool} &
        \textbf{Code-Qualität} &
        \textbf{Korrektheit} &
        \textbf{Anpassungs\-aufwand} &
        \textbf{Kontext\-verständnis} &
        \textbf{Zeitersparnis} \\
        \midrule
        ChatGPT & 4 & 4 & 3 & 3 & 4 \\
        Grok (xAI) & 4 & 3 & 3 & 3 & 3 \\
        DeepSeek & 3 & 3 & 2 & 2 & 3 \\
        Cursor & \textbf{5} & \textbf{5} & \textbf{4} & \textbf{5} & \textbf{5} \\
        \bottomrule
    \end{tabularx}
    \caption{Bewertung der AI-Tools anhand definierter Kriterien (1 = schlecht, 5 = sehr gut)}
    \label{tab:ai-bewertung}
\end{table}

\subsubsection*{Fazit der Auswertung}

Auf Basis der definierten Kriterien und der praktischen Nutzung im Projekt
konnte Cursor als klarer Gewinner identifiziert werden. Ausschlaggebend war
insbesondere das sehr gute Kontextverständnis durch den Zugriff auf die gesamte
Projektstruktur sowie der daraus resultierende deutliche Zeitgewinn im
Projektalltag.

Während sich die zugrunde liegenden Sprachmodelle in ihrer grundsätzlichen
Qualität nur begrenzt unterscheiden, bietet Cursor durch sein IDE-nahes
Frontend einen entscheidenden Mehrwert. Für unser Projekt erwies sich diese
Kombination aus starkem Modell und vollständigem Projektkontext als die
effektivste Lösung für einen konsequenten AI-First-Ansatz.

\subsection{Detaillierter Fragen zu Cursor}
Da wir von Cursor sehr überzeugt sind, insbesondere weil es sich um eine
Applikation mit eigenem Frontend handelt, können ganze Ordnerstrukturen
geöffnet und analysiert werden, ähnlich wie in Visual Studio Code. Dadurch
erhält die KI einen direkten Überblick über die gesamte Codebasis. Dies
erleichtert es deutlich, präzisere und sinnvollere Antworten zu erhalten, da
der Kontext einzelner Codestellen für die KI jederzeit klar ist. Im Gegensatz
zu klassischen webbasierten Tools, bei denen nur selektiv hochgeladener Code
betrachtet wird, entstehen so weniger Fehlinterpretationen.

Nachfolgend ist ein Screenshot dargestellt, der exemplarisch zeigt, wie gut
eine einzelne Datei analysiert werden kann, wenn der vollständige
Projektkontext vorhanden ist. In diesem Beispiel wurde die KI gefragt, welche
Aufgabe die Datei \texttt{CommonApplicationBaseModuleLoader.swift} übernimmt
und wie sie im Zusammenhang mit dem restlichen Projekt steht. Dabei wird
deutlich, dass Cursor die Projektstruktur ganzheitlich analysiert und
relevante Codestellen gezielt hervorhebt, was das Verständnis komplexer
Zusammenhänge erheblich erleichtert.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{Fotos/Cursor_1.png}
  \caption{Analysebeispiel in Cursor}
  \label{fig:cursor-example}
\end{figure}


\subsection{Fazit}

Auf Basis der durchgeführten Evaluation und der praktischen Erfahrungen im
Projekt sind wir zum Schluss gekommen, dass Cursor in Kombination mit einem
der evaluierten Sprachmodelle für unseren Anwendungsfall die beste Lösung
darstellt. Ausschlaggebend war insbesondere der Zugriff auf den vollständigen
Projektkontext, der es der KI ermöglicht, deutlich sinnvolleren und besser
integrierten Code zu erzeugen als klassische webbasierten Tools.

Die zugrunde liegenden Large Language Models unterscheiden sich aus unserer
Sicht im Kern weniger stark voneinander, als es auf den ersten Blick scheint.
Der entscheidende Mehrwert entsteht vielmehr durch das Umfeld, in dem das
Modell eingesetzt wird. Hier bietet Cursor durch sein IDE-nahes Frontend klare
Vorteile im Projektalltag.

Der Cursor Pro Plan erweist sich dabei grundsätzlich als ausreichend, da im
Auto-Modus für kleinere Anfragen kostengünstige oder kostenfreie APIs genutzt
werden und kostenpflichtige Modelle nur bei umfangreicheren Abfragen zum
Einsatz kommen. Dennoch ist zu beachten, dass bei intensiver Nutzung oder bei
mehreren gleichzeitig arbeitenden Personen zusätzliche Kosten entstehen
können.

Im weiteren Projektverlauf wurde zudem Rücksprache mit dem Dozenten gehalten,
welche AI-Tools seitens der Hochschule Luzern bevorzugt werden. Dabei zeigte
sich, dass ChatGPT offiziell unterstützt wird. Um diese Vorgabe einzuhalten
und gleichzeitig nicht auf die Vorteile von Cursor verzichten zu müssen,
wurde Cursor so konfiguriert, dass ausschliesslich die ChatGPT-API verwendet
wird. Dadurch konnte ein sinnvoller Kompromiss gefunden werden, der sowohl
den institutionellen Rahmenbedingungen als auch den praktischen Anforderungen
des Projekts gerecht wird.

\subsection{Erfahrungen mit dem AI-First-Ansatz}

Während der gesamten Projektlaufzeit wurde der \emph{AI-First}-Ansatz
konsequent verfolgt und Cursor aktiv in den Entwicklungsprozess integriert.
Dabei konnten sowohl positive Effekte als auch klare Grenzen des Einsatzes
von KI beobachtet werden. Insgesamt hat sich gezeigt, dass AI den
Entwicklungsprozess deutlich unterstützen und beschleunigen kann, jedoch
weiterhin eine kritische Bewertung und fachliche Kontrolle durch die
Entwickler notwendig bleibt.

Eine detaillierte Reflexion der gemachten Erfahrungen, insbesondere zu
Stärken, Schwächen und Grenzen von AI-gestützter Entwicklung, erfolgt im
Kapitel zur Reflexion (siehe Abschnitt~\ref{sec:ai-reflexion}).
